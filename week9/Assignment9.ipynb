{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 9\n",
    "\n",
    "### Shovan Biswas, Ben Horvath, Randy Thompson"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Set Information\n",
    "\n",
    "Source: http://archive.ics.uci.edu/ml/datasets/Spambase\n",
    "\n",
    "_Our collection of spam e-mails came from our postmaster and individuals who had filed spam. Our collection of non-spam e-mails came from filed work and personal e-mails, and hence the word 'george' and the area code '650' are indicators of non-spam. These are useful when constructing a personalized spam filter. One would either have to blind such non-spam indicators or get a very wide collection of non-spam to generate a general purpose spam filter._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From source website, we donwloaded the files `spambase.names` and `spambase.data`. The latter contains the actual data and the former contains the column names and directions to construct the file. Following the directions, we created spambase.csv.    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and read    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree\n",
    "from sklearn import ensemble\n",
    "from sklearn import svm\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "import sklearn.metrics as sm\n",
    "\n",
    "import seaborn as sn\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "spam_data = pd.read_csv(\"spambase.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic data exploration   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 4601 rows, where each row corresponds to an e-mail. Each e-mail is described by fifty-seven features, and annotated with a class name (spam/not spam). The variables contain the porportion of the words in the e-mail that are a particular token or character, e.g., exclamation mark, the word 'address.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spamclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "0            0.00               0.64           0.64           0.0   \n",
       "1            0.21               0.28           0.50           0.0   \n",
       "2            0.06               0.00           0.71           0.0   \n",
       "3            0.00               0.00           0.00           0.0   \n",
       "4            0.00               0.00           0.00           0.0   \n",
       "\n",
       "   word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "0           0.32            0.00              0.00                0.00   \n",
       "1           0.14            0.28              0.21                0.07   \n",
       "2           1.23            0.19              0.19                0.12   \n",
       "3           0.63            0.00              0.31                0.63   \n",
       "4           0.63            0.00              0.31                0.63   \n",
       "\n",
       "   word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "0             0.00            0.00  ...         0.00        0.000   \n",
       "1             0.00            0.94  ...         0.00        0.132   \n",
       "2             0.64            0.25  ...         0.01        0.143   \n",
       "3             0.31            0.63  ...         0.00        0.137   \n",
       "4             0.31            0.63  ...         0.00        0.135   \n",
       "\n",
       "   char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "0          0.0        0.778        0.000        0.000   \n",
       "1          0.0        0.372        0.180        0.048   \n",
       "2          0.0        0.276        0.184        0.010   \n",
       "3          0.0        0.137        0.000        0.000   \n",
       "4          0.0        0.135        0.000        0.000   \n",
       "\n",
       "   capital_run_length_average  capital_run_length_longest  \\\n",
       "0                       3.756                          61   \n",
       "1                       5.114                         101   \n",
       "2                       9.821                         485   \n",
       "3                       3.537                          40   \n",
       "4                       3.537                          40   \n",
       "\n",
       "   capital_run_length_total  spamclass  \n",
       "0                       278          1  \n",
       "1                      1028          1  \n",
       "2                      2259          1  \n",
       "3                       191          1  \n",
       "4                       191          1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Content of this dataset: 4601 rows and 58 columns\n"
     ]
    }
   ],
   "source": [
    "print(\"Content of this dataset: \" + str(spam_data.shape[0]) + \" rows and \" + str(spam_data.shape[1]) + \" columns\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Column wise statistics of data    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spamclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "      <td>4601.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.104553</td>\n",
       "      <td>0.213015</td>\n",
       "      <td>0.280656</td>\n",
       "      <td>0.065425</td>\n",
       "      <td>0.312223</td>\n",
       "      <td>0.095901</td>\n",
       "      <td>0.114208</td>\n",
       "      <td>0.105295</td>\n",
       "      <td>0.090067</td>\n",
       "      <td>0.239413</td>\n",
       "      <td>...</td>\n",
       "      <td>0.038575</td>\n",
       "      <td>0.139030</td>\n",
       "      <td>0.016976</td>\n",
       "      <td>0.269071</td>\n",
       "      <td>0.075811</td>\n",
       "      <td>0.044238</td>\n",
       "      <td>5.191515</td>\n",
       "      <td>52.172789</td>\n",
       "      <td>283.289285</td>\n",
       "      <td>0.394045</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.305358</td>\n",
       "      <td>1.290575</td>\n",
       "      <td>0.504143</td>\n",
       "      <td>1.395151</td>\n",
       "      <td>0.672513</td>\n",
       "      <td>0.273824</td>\n",
       "      <td>0.391441</td>\n",
       "      <td>0.401071</td>\n",
       "      <td>0.278616</td>\n",
       "      <td>0.644755</td>\n",
       "      <td>...</td>\n",
       "      <td>0.243471</td>\n",
       "      <td>0.270355</td>\n",
       "      <td>0.109394</td>\n",
       "      <td>0.815672</td>\n",
       "      <td>0.245882</td>\n",
       "      <td>0.429342</td>\n",
       "      <td>31.729449</td>\n",
       "      <td>194.891310</td>\n",
       "      <td>606.347851</td>\n",
       "      <td>0.488698</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.588000</td>\n",
       "      <td>6.000000</td>\n",
       "      <td>35.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.065000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>2.276000</td>\n",
       "      <td>15.000000</td>\n",
       "      <td>95.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.420000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.380000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.160000</td>\n",
       "      <td>...</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.188000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.315000</td>\n",
       "      <td>0.052000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>3.706000</td>\n",
       "      <td>43.000000</td>\n",
       "      <td>266.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.540000</td>\n",
       "      <td>14.280000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>42.810000</td>\n",
       "      <td>10.000000</td>\n",
       "      <td>5.880000</td>\n",
       "      <td>7.270000</td>\n",
       "      <td>11.110000</td>\n",
       "      <td>5.260000</td>\n",
       "      <td>18.180000</td>\n",
       "      <td>...</td>\n",
       "      <td>4.385000</td>\n",
       "      <td>9.752000</td>\n",
       "      <td>4.081000</td>\n",
       "      <td>32.478000</td>\n",
       "      <td>6.003000</td>\n",
       "      <td>19.829000</td>\n",
       "      <td>1102.500000</td>\n",
       "      <td>9989.000000</td>\n",
       "      <td>15841.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       word_freq_make  word_freq_address  word_freq_all  word_freq_3d  \\\n",
       "count     4601.000000        4601.000000    4601.000000   4601.000000   \n",
       "mean         0.104553           0.213015       0.280656      0.065425   \n",
       "std          0.305358           1.290575       0.504143      1.395151   \n",
       "min          0.000000           0.000000       0.000000      0.000000   \n",
       "25%          0.000000           0.000000       0.000000      0.000000   \n",
       "50%          0.000000           0.000000       0.000000      0.000000   \n",
       "75%          0.000000           0.000000       0.420000      0.000000   \n",
       "max          4.540000          14.280000       5.100000     42.810000   \n",
       "\n",
       "       word_freq_our  word_freq_over  word_freq_remove  word_freq_internet  \\\n",
       "count    4601.000000     4601.000000       4601.000000         4601.000000   \n",
       "mean        0.312223        0.095901          0.114208            0.105295   \n",
       "std         0.672513        0.273824          0.391441            0.401071   \n",
       "min         0.000000        0.000000          0.000000            0.000000   \n",
       "25%         0.000000        0.000000          0.000000            0.000000   \n",
       "50%         0.000000        0.000000          0.000000            0.000000   \n",
       "75%         0.380000        0.000000          0.000000            0.000000   \n",
       "max        10.000000        5.880000          7.270000           11.110000   \n",
       "\n",
       "       word_freq_order  word_freq_mail  ...  char_freq_;  char_freq_(  \\\n",
       "count      4601.000000     4601.000000  ...  4601.000000  4601.000000   \n",
       "mean          0.090067        0.239413  ...     0.038575     0.139030   \n",
       "std           0.278616        0.644755  ...     0.243471     0.270355   \n",
       "min           0.000000        0.000000  ...     0.000000     0.000000   \n",
       "25%           0.000000        0.000000  ...     0.000000     0.000000   \n",
       "50%           0.000000        0.000000  ...     0.000000     0.065000   \n",
       "75%           0.000000        0.160000  ...     0.000000     0.188000   \n",
       "max           5.260000       18.180000  ...     4.385000     9.752000   \n",
       "\n",
       "       char_freq_[  char_freq_!  char_freq_$  char_freq_#  \\\n",
       "count  4601.000000  4601.000000  4601.000000  4601.000000   \n",
       "mean      0.016976     0.269071     0.075811     0.044238   \n",
       "std       0.109394     0.815672     0.245882     0.429342   \n",
       "min       0.000000     0.000000     0.000000     0.000000   \n",
       "25%       0.000000     0.000000     0.000000     0.000000   \n",
       "50%       0.000000     0.000000     0.000000     0.000000   \n",
       "75%       0.000000     0.315000     0.052000     0.000000   \n",
       "max       4.081000    32.478000     6.003000    19.829000   \n",
       "\n",
       "       capital_run_length_average  capital_run_length_longest  \\\n",
       "count                 4601.000000                 4601.000000   \n",
       "mean                     5.191515                   52.172789   \n",
       "std                     31.729449                  194.891310   \n",
       "min                      1.000000                    1.000000   \n",
       "25%                      1.588000                    6.000000   \n",
       "50%                      2.276000                   15.000000   \n",
       "75%                      3.706000                   43.000000   \n",
       "max                   1102.500000                 9989.000000   \n",
       "\n",
       "       capital_run_length_total    spamclass  \n",
       "count               4601.000000  4601.000000  \n",
       "mean                 283.289285     0.394045  \n",
       "std                  606.347851     0.488698  \n",
       "min                    1.000000     0.000000  \n",
       "25%                   35.000000     0.000000  \n",
       "50%                   95.000000     0.000000  \n",
       "75%                  266.000000     1.000000  \n",
       "max                15841.000000     1.000000  \n",
       "\n",
       "[8 rows x 58 columns]"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Spam / non-spam count     \n",
    "\n",
    "The last column spamclass determines whether a data is spam or otherwise. If spamclass is 1, it's a spam and if it's 0, it's not a spam. (Ref: attribute descriptions in spambase.names).       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spam data count: 1813\n",
      "Non spam data count: 2788\n"
     ]
    }
   ],
   "source": [
    "spam_column = len(spam_data[spam_data.spamclass == 1])\n",
    "notspam_column = len(spam_data[spam_data.spamclass == 0])\n",
    "\n",
    "print(\"Spam data count: %d\" %spam_column)\n",
    "print(\"Non spam data count: %d\" %notspam_column)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check for nulls \n",
    "\n",
    "Forunately for us, there are no nulls in the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>word_freq_make</th>\n",
       "      <th>word_freq_address</th>\n",
       "      <th>word_freq_all</th>\n",
       "      <th>word_freq_3d</th>\n",
       "      <th>word_freq_our</th>\n",
       "      <th>word_freq_over</th>\n",
       "      <th>word_freq_remove</th>\n",
       "      <th>word_freq_internet</th>\n",
       "      <th>word_freq_order</th>\n",
       "      <th>word_freq_mail</th>\n",
       "      <th>...</th>\n",
       "      <th>char_freq_;</th>\n",
       "      <th>char_freq_(</th>\n",
       "      <th>char_freq_[</th>\n",
       "      <th>char_freq_!</th>\n",
       "      <th>char_freq_$</th>\n",
       "      <th>char_freq_#</th>\n",
       "      <th>capital_run_length_average</th>\n",
       "      <th>capital_run_length_longest</th>\n",
       "      <th>capital_run_length_total</th>\n",
       "      <th>spamclass</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>0 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Empty DataFrame\n",
       "Columns: [word_freq_make, word_freq_address, word_freq_all, word_freq_3d, word_freq_our, word_freq_over, word_freq_remove, word_freq_internet, word_freq_order, word_freq_mail, word_freq_receive, word_freq_will, word_freq_people, word_freq_report, word_freq_addresses, word_freq_free, word_freq_business, word_freq_email, word_freq_you, word_freq_credit, word_freq_your, word_freq_font, word_freq_000, word_freq_money, word_freq_hp, word_freq_hpl, word_freq_george, word_freq_650, word_freq_lab, word_freq_labs, word_freq_telnet, word_freq_857, word_freq_data, word_freq_415, word_freq_85, word_freq_technology, word_freq_1999, word_freq_parts, word_freq_pm, word_freq_direct, word_freq_cs, word_freq_meeting, word_freq_original, word_freq_project, word_freq_re, word_freq_edu, word_freq_table, word_freq_conference, char_freq_;, char_freq_(, char_freq_[, char_freq_!, char_freq_$, char_freq_#, capital_run_length_average, capital_run_length_longest, capital_run_length_total, spamclass]\n",
       "Index: []\n",
       "\n",
       "[0 rows x 58 columns]"
      ]
     },
     "execution_count": 129,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spam_data[spam_data.isnull().any(axis = 1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation\n",
    "\n",
    "We'll split the data into train, validation, and test sets, with 70 percent in train.         "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_percentage = 0.7\n",
    "val_percentage = 0.15\n",
    "\n",
    "spam_data_rows = len(spam_data)\n",
    "train_rows = int(spam_data_rows * train_percentage)\n",
    "val_rows = int(spam_data_rows * val_percentage)\n",
    "test_rows = spam_data_rows - train_rows - val_rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training rows (70 prc of total): 3220\n",
      "Validation rows (15 prc of total): 690\n",
      "Testing rows (15 prc of total): 691\n",
      "Total: 4601\n"
     ]
    }
   ],
   "source": [
    "print(\"Training rows (70 prc of total): %d\" %train_rows)\n",
    "print(\"Validation rows (15 prc of total): %d\" %val_rows)\n",
    "print(\"Testing rows (15 prc of total): %d\" %test_rows)\n",
    "print(\"Total: %d\" %(train_rows + val_rows + test_rows))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, test_set = train_test_split(spam_data, test_size = test_rows, random_state = 8)\n",
    "train_set, val_set = train_test_split(train_set, test_size = val_rows, random_state = 88)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set: 3220\n",
      "Validation set: 690\n",
      "Testing set: 691\n",
      "Total: 4601\n"
     ]
    }
   ],
   "source": [
    "print(\"Training set: %d\" %len(train_set))\n",
    "print(\"Validation set: %d\" %len(val_set))\n",
    "print(\"Testing set: %d\" %len(test_set))\n",
    "print(\"Total: %d\" %(len(train_set) + len(val_set) + len(test_set)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion Matrix\n",
    "\n",
    "The following function (just a function, no action is happening here) computes Confusion Matrix, which shows True Positives (TP), True Negatives (TN), False Positives (FP) and False Negatives (FN), from which the performance of an algorithm can be determined.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func_confusion_matrix(y_true, y_pred):\n",
    "    cm = sm.confusion_matrix(y_true, y_pred, labels = [1, 0])\n",
    "    print(\"TP: %d\" %cm[0,0])\n",
    "    print(\"FP: %d\" %cm[1,0])\n",
    "    print(\"TN: %d\" %cm[1,1])\n",
    "    print(\"FN: %d\" %cm[0,1])\n",
    "    print(sm.classification_report(y_true, y_pred, labels = [1,0], target_names = [\"Spam\", \"Not spam\"]))  \n",
    "    return cm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll classify the data in three possible machine learning methods, namely Decision Tree, Random Forest and Support Vector Machines.      "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Decision Tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we'll classify using decision tree, where each node represents a portion of the data, whether it's spam or not.    \n",
    "\n",
    "__We will use 10-fold cross validation to keep the learning algorithm from overfitting, to provide the best estimate of classifier performance possible, as well as hyperparameter tuning.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification and model statistics for training set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prepare the training data for input to the algorith:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_class = train_set['spamclass']\n",
    "train_vars = train_set.drop(labels = 'spamclass', axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, set the infastructure for hyperparameter tuning on the train set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 25, 'min_samples_split': 10}\n",
      "TP: 1230\n",
      "FP: 17\n",
      "TN: 1941\n",
      "FN: 32\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Spam       0.99      0.97      0.98      1262\n",
      "    Not spam       0.98      0.99      0.99      1958\n",
      "\n",
      "   micro avg       0.98      0.98      0.98      3220\n",
      "   macro avg       0.99      0.98      0.98      3220\n",
      "weighted avg       0.98      0.98      0.98      3220\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1230,   32],\n",
       "       [  17, 1941]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import recall_score, make_scorer\n",
    "\n",
    "recall_scorer = make_scorer(recall_score)\n",
    "\n",
    "param_grid = {'criterion': ['gini', 'entropy'],\n",
    "              'max_depth': [2, 5, 10, 15, 25, 30],\n",
    "              'min_samples_split': [10, 20, 50, 100]}\n",
    "\n",
    "dt = tree.DecisionTreeClassifier()\n",
    "\n",
    "# Instantiate grid search and tell it to maximize recall\n",
    "dt_cv = GridSearchCV(dt, param_grid, cv=10, scoring=recall_scorer)\n",
    "\n",
    "# Fit it to the data, and get info about the best model\n",
    "dt_cv.fit(train_vars, train_class)\n",
    "print(dt_cv.best_params_)\n",
    "\n",
    "# Evaluate\n",
    "dt_train = dt_cv.predict(train_vars)\n",
    "func_confusion_matrix(train_class, dt_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The output above is very informative.\n",
    "\n",
    "First, the optimal parameters of the decision tree are output. Second, is the classification matrix. And third shows detailed performance information.\n",
    "\n",
    "__Particularly important to us is recall, which are optimizing for.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification and model statistics for test set\n",
    "\n",
    "The performance on the test set below is comparable to the cross-validated performance on the train set -- although the model is perhaps slightly overfitted.\n",
    "\n",
    "_Results:_ We can say that the trained decision tree correctly identifies spam as spam about 91 percent of the time (precision), and correctly identifies non-spam as non-span 88 percent of th etime (recall)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 244\n",
      "FP: 29\n",
      "TN: 383\n",
      "FN: 35\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Spam       0.89      0.87      0.88       279\n",
      "    Not spam       0.92      0.93      0.92       412\n",
      "\n",
      "   micro avg       0.91      0.91      0.91       691\n",
      "   macro avg       0.91      0.90      0.90       691\n",
      "weighted avg       0.91      0.91      0.91       691\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[244,  35],\n",
       "       [ 29, 383]])"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_class = test_set['spamclass']\n",
    "test_vars = test_set.drop(labels='spamclass', axis = 1)\n",
    "\n",
    "# Evaluate\n",
    "dt_test = dt_cv.predict(test_vars)\n",
    "func_confusion_matrix(test_class, dt_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Determine feature importance\n",
    "\n",
    "We can also go 'under the hood' of the model, to see what it computed are the most important variables. Note that the measure of importance, `imp`, is standarded to be between zero and one.\n",
    "\n",
    "We see the frequency of the exclamation mark character (!) is by far the most indicative of a spam e-mail. The dollar sign character ($) as well as the word 'remove' form a second set of importance features. Then there appears to be a 'long tail' of less predictive words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>char_freq_!</td>\n",
       "      <td>0.306203</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>char_freq_$</td>\n",
       "      <td>0.136087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>word_freq_remove</td>\n",
       "      <td>0.130127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>capital_run_length_average</td>\n",
       "      <td>0.056622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>capital_run_length_longest</td>\n",
       "      <td>0.050549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>word_freq_george</td>\n",
       "      <td>0.033713</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>capital_run_length_total</td>\n",
       "      <td>0.027828</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>word_freq_our</td>\n",
       "      <td>0.027438</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>word_freq_free</td>\n",
       "      <td>0.023608</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>word_freq_hp</td>\n",
       "      <td>0.022861</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      variable       imp\n",
       "51                 char_freq_!  0.306203\n",
       "52                 char_freq_$  0.136087\n",
       "6             word_freq_remove  0.130127\n",
       "54  capital_run_length_average  0.056622\n",
       "55  capital_run_length_longest  0.050549\n",
       "26            word_freq_george  0.033713\n",
       "56    capital_run_length_total  0.027828\n",
       "4                word_freq_our  0.027438\n",
       "15              word_freq_free  0.023608\n",
       "24                word_freq_hp  0.022861"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def func_arrange_feature_by_importance(fit, vars):\n",
    "    df = {'variable': pd.Series(vars.columns.values), 'imp': pd.Series(fit.feature_importances_)}\n",
    "    return pd.DataFrame(df, columns=['variable','imp']).sort_values(['imp'], ascending=0).head(10)\n",
    "\n",
    "# use best_estimator_ to access the best performaing cross validated model\n",
    "func_arrange_feature_by_importance(dt_cv.best_estimator_, test_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest\n",
    "\n",
    "Random Forest algorithm decreases the variance of a single decision tree. While the latter are implemented on an entire dataset, using all features (variables) of interest, a Random Forest randomly selects observations (i.e. rows) and specific features to build multiple Decision Trees, the computing an average measure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification and model statistics for training set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10, 'n_estimators': 600}\n",
      "TP: 1235\n",
      "FP: 11\n",
      "TN: 1947\n",
      "FN: 27\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Spam       0.99      0.98      0.98      1262\n",
      "    Not spam       0.99      0.99      0.99      1958\n",
      "\n",
      "   micro avg       0.99      0.99      0.99      3220\n",
      "   macro avg       0.99      0.99      0.99      3220\n",
      "weighted avg       0.99      0.99      0.99      3220\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1235,   27],\n",
       "       [  11, 1947]])"
      ]
     },
     "execution_count": 139,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  to speed up training, plug in results from above:\n",
    "#{'criterion': 'entropy', 'max_depth': 30, 'min_samples_split': 10}\n",
    "\n",
    "param_grid = {'criterion': ['entropy'],\n",
    "              'max_depth': [30],\n",
    "              'min_samples_split': [10],\n",
    "              'n_estimators': [500, 600, 700]}\n",
    "\n",
    "forest = ensemble.RandomForestClassifier()\n",
    "\n",
    "# Instantiate grid search and tell it to maximize recall\n",
    "rf_cv = GridSearchCV(forest, param_grid, cv=10, scoring=recall_scorer)\n",
    "\n",
    "# Fit it to the data, and get info about the best model\n",
    "rf_cv.fit(train_vars, train_class)\n",
    "print(rf_cv.best_params_)\n",
    "\n",
    "# Evaluate\n",
    "rf_train = rf_cv.predict(train_vars)\n",
    "func_confusion_matrix(train_class, rf_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best performing Random Forest uses 600 trees."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Classification and model statistics for test set\n",
    "\n",
    "The below shows how well this model performs on the test set. It appears to perform better than a decision tree alone. Only 33 observations of over 4000 are misclassified!\n",
    "\n",
    "96 percent of spam e-mails are correctly classified as spam e-mails, and 92 percent of non-spam are correctly classified as non-spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 258\n",
      "FP: 12\n",
      "TN: 400\n",
      "FN: 21\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Spam       0.96      0.92      0.94       279\n",
      "    Not spam       0.95      0.97      0.96       412\n",
      "\n",
      "   micro avg       0.95      0.95      0.95       691\n",
      "   macro avg       0.95      0.95      0.95       691\n",
      "weighted avg       0.95      0.95      0.95       691\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[258,  21],\n",
       "       [ 12, 400]])"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_test = rf_cv.predict(test_vars)\n",
    "func_confusion_matrix(test_class, rf_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, we can determine the feature importance according to the Random Forest. Like the individual decision trees, the exclamation and dollars signs are the most importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>variable</th>\n",
       "      <th>imp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>char_freq_!</td>\n",
       "      <td>0.119229</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>char_freq_$</td>\n",
       "      <td>0.086826</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>word_freq_remove</td>\n",
       "      <td>0.083747</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>capital_run_length_average</td>\n",
       "      <td>0.062230</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>word_freq_free</td>\n",
       "      <td>0.056850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>capital_run_length_longest</td>\n",
       "      <td>0.055157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>word_freq_your</td>\n",
       "      <td>0.055032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>word_freq_hp</td>\n",
       "      <td>0.053533</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>capital_run_length_total</td>\n",
       "      <td>0.043057</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>word_freq_money</td>\n",
       "      <td>0.034617</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                      variable       imp\n",
       "51                 char_freq_!  0.119229\n",
       "52                 char_freq_$  0.086826\n",
       "6             word_freq_remove  0.083747\n",
       "54  capital_run_length_average  0.062230\n",
       "15              word_freq_free  0.056850\n",
       "55  capital_run_length_longest  0.055157\n",
       "20              word_freq_your  0.055032\n",
       "24                word_freq_hp  0.053533\n",
       "56    capital_run_length_total  0.043057\n",
       "23             word_freq_money  0.034617"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "func_arrange_feature_by_importance(rf_cv.best_estimator_, test_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Support Vector Machines\n",
    "\n",
    "Support Vector Machine (SVM) is a machine learning algorithms that is used to separate sets of points in a hyperspace, by a hyperplain. Train the algorithm:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'C': 100, 'gamma': 0.001}\n",
      "TP: 1206\n",
      "FP: 38\n",
      "TN: 1920\n",
      "FN: 56\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Spam       0.97      0.96      0.96      1262\n",
      "    Not spam       0.97      0.98      0.98      1958\n",
      "\n",
      "   micro avg       0.97      0.97      0.97      3220\n",
      "   macro avg       0.97      0.97      0.97      3220\n",
      "weighted avg       0.97      0.97      0.97      3220\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[1206,   56],\n",
       "       [  38, 1920]])"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import svm\n",
    "\n",
    "param_grid = {'C': [0.1, 1, 10, 100],\n",
    "              'gamma': [1, 0.1, 0.01, 0.001]}\n",
    "\n",
    "# Instantiate grid search and tell it to maximize recall\n",
    "svm_cv = GridSearchCV(svm.SVC(), param_grid, cv=5, scoring=recall_scorer)\n",
    "\n",
    "# Fit it to the data, and get info about the best model\n",
    "svm_cv.fit(train_vars, train_class)\n",
    "print(svm_cv.best_params_)\n",
    "\n",
    "# Evaluate\n",
    "svm_train = svm_cv.predict(train_vars)\n",
    "func_confusion_matrix(train_class, svm_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, evalaute on the train set. We see that this particular SVM performs poorly compared to a decision tree or the random forest -- though this is probably because of sub-optimal hyper-tuning. \n",
    "\n",
    "Still, in the grand scheme of things this would be considered good performance. 86 percent of spam e-mails are correctly classified as spam, and 88 percent of non-spam e-mails are classified as non-spam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TP: 245\n",
      "FP: 39\n",
      "TN: 373\n",
      "FN: 34\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "        Spam       0.86      0.88      0.87       279\n",
      "    Not spam       0.92      0.91      0.91       412\n",
      "\n",
      "   micro avg       0.89      0.89      0.89       691\n",
      "   macro avg       0.89      0.89      0.89       691\n",
      "weighted avg       0.89      0.89      0.89       691\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[245,  34],\n",
       "       [ 39, 373]])"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "svm_test = svm_cv.predict(test_vars)\n",
    "func_confusion_matrix(test_class, svm_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
